\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\usepackage{biblatex}

\addbibresource{ref.bib}

\title{Thesis Proposal Review}
%\author{sean.a.lamont }
%\date{April 2022}


\begin{document}

    \maketitle


    \section{Overview}\label{sec:overview}
    This research focuses on the automation of Interactive Theorem Proving (ITP) systems through Artificial Intelligence and Machine Learning.
    ITP systems are central to modern formal verification efforts as they allow for more complex proofs than traditional Automated Theorem Proving (ATP) approaches\cite{nawaz_survey_2019, harrison_history_2014}.
    ATP systems generally consist of fully automated heuristic based search algorithms which scale poorly with the complexity of the task.
    ITP systems instead require a human to guide the proof system at a higher level of abstraction, and can utilise ATPs for proving smaller, more tractable subgoals.
    Proof guidance requires human expertise in both the area of reasoning and the proving system itself.
    This limits the scalability of these approaches, with large scale formalisation projects taking many years to complete\cite{wiedijk_bruijn_2000}
    As a result, progress in the automation of this proof guidance could improve the scalability of formal verification approaches.  \\


%There is an important distinction to be made between the automation of ITP (which we call "AITP"), and what has historically been referred to as "Automated Theorem Proving" (which we refer to as "ATP").


    \section{Literature Review}\label{sec:literature-review}
    The application of AI and Machine Learning methods to Mathematical Reasoning is an emerging research area.
    There are different paradigms within this, and\cite{lu_survey_nodate} presents a comprehensive overview with a summary of progress to date.
    The scope of this project, at present, is on the application of AI/ML methods to Interactive Theorem Proving (ITP). \\

    \subsection{Interactive Theorem Proving}\label{subsec:interactive-theorem-proving}
    Interactive Theorem Proving (ITP) is a method of formal verification where a human guides a proof system at a high level.
    Given a goal to prove, the human selects a \textit{tactic} to apply to the goal, which defines the strategy to use.
    The tactic can also include \textit{arguments}, which are usually previously proven theorems (\textit{premises}) or variables in the goal (\textit{terms})\footnote{Arbitrary arguments can also be provided, although in the context of AITP we usually restrict the arguments to premises and tactics.}.
    The system then attempts to prove the statement with the provided tactic.
    It returns either an empty list indicating the goal has been proven, or a list of subgoals, the conjunction of which is equivalent to the original goal.
    Repeating this process leads to a proof tree, with leaves being goals and edges the tactics which generated them.
    The selection of a goal to prove from this tree is a key part of ITP guidance.

    \subsubsection{Systems}
    There are many ITP systems currently available.
    For our purposes, the main systems used to investigate ITP automation are HOL4~\cite{slind_brief_2008}, HOL Light~\cite{harrison_hol_2009}, Coq\cite{paulin-mohring_introduction_2012}, Isabelle/HOL\cite{paulson_isabelle_1994}, Lean\cite{felty_lean_2015} and MetaMath\cite{megill_metamath_2019}

    \subsubsection{Applications}
    ITP has been an essential tool for many large scale formal verification efforts.
    Some of the most noteworthy applications include:

    \begin{itemize}
        \item Fully verified compilers:
        The CakeML\cite{tan_verified_2019} project includes a verified compiler for Standard ML\cite{milner_definition_1997}, a functional programming language.
        CompCert\cite{leroy_compcert_2014} is a C compiler which was verified using the Coq ITP\@.
        It has been used in safety critical applications such as flight controllers\cite{franca_formally_2012}.
        \item seL4\cite{klein_sel4_2009} is a microkernel verified using Isabelle/HOL and HOL4\cite{heiser_sel4_2020}.
        It has been used by NASA in their High Performance Spaceflight Computing (HPSC) program\cite{noauthor_nasa_nodate}, among other high integrity applications~\cite{vanderleest_is_2018,matos_sel4_2022,heiser_sel4_australia_2020}.
        \item Verified correct vote-counting for large scale elections\cite{moses_no_2017}
        \item Formalising pure mathematics.
        Some key examples are a formal proof of the Kepler Conjecture\cite{hales_formal_2017} and the four colour theorem\cite{gonthier_four_2008}.
        \item Provably correct hardware design\cite{gupta_formal_1992,kern_formal_2002}
    \end{itemize}

    \subsection{Current results in Automating ITP}\label{subsec:current-results-in-automating-itp}

    \subsubsection{Datasets and benchmarks}
    Progress in the area of ITP automation has been measured with benchmarks spanning several ITP frameworks.
    These benchmarks either measure direct proof performance through an ITP interface, or use datasets constructed from ITP proof traces.
    These proof traces are used to construct supervised learning tasks, such as premise selection or tactic prediction. \\

    As discussed in~\cite{lu_survey_nodate}, current benchmarks include
    \begin{itemize}
        \item HOList~\cite{bansal_holist_2019}
        \item HOLStep~\cite{kaliszyk_holstep_2017}
        \item MIZAR~\cite{jakubuv_mizar_2023, grabowski_four_2015}
        \item CoqGym~\cite{yang_learning_2019}
        \item PISA~\cite{jiang_lisa_2021}
        \item LeanStep~\cite{han_proof_2021}
        \item Lean-Gym~\cite{polu_formal_2022}
        \item mini-F2F~\cite{zheng_minif2f_2021}
        \item INT~\cite{wu_int_2020}
        \item IsarStep~\cite{li_isarstep_2020}
        \item GamePad~\cite{huang_gamepad_2018}
    \end{itemize}

    \subsubsection{Approaches}

%    The high level goal for automating ITP is straightforward: Given a goal, generate a valid proof in the ITP system.
%    Despite this apparent simplicity, the approaches used in the literature vary greatly.
%
%    Expression modelling
%        Graph (LSTM, GNN)
%        Sequence (LSTM, Tree LSTM, Transformer)
%
%    Upstream task
%        Premise selection
%        Tactic Selection
%
%    Proof guidance
%        BFS
%        Fringe
%        MCTS
%
%    Learning approach
%        Supervised
%        Semi Supervised
%        Fine-tune
%        End to end RL
%
%    Embedding + upstream task, or GPT decoder for direct prediction
%
%
%    Proof guidance (hypertree, bfs, fringes),
%    supervised learning for model + guidance (google + meta),
%    end to end RL (tacticzero)
%
%    LLMs (Greedy reasoning, poor at proof search, premise selection done in natural language, harder than explicit pairwise comparision)
%
%    Direct interaction is a more representative task, but can create a performance bottleneck due to the required environment interaction.
%    Given the number of ITPs available, like for like comparison between approaches is difficult.
%


    \section{Proposed Research}\label{sec:proposed-research}

    \subsection{Comparison of Embedding Architectures}\label{subsec:comparison-of-embedding-architectures}

    \subsection{Improving Proof Guidance}\label{subsec:proof-guidance}

    \subsection{Open Source Experimentation Framework}\label{subsec:open-source-experimentation-framework}
    \begin{itemize}
        \item AITP large field. Subproblem automated ITP.
        \item many separate benchmarks across different ITPs.
        \item many separate models and approaches.
        \item Some approaches not openly available
        \item Want to be able to compare approaches easily between different ITP domains
        \begin{itemize}
            \item Goal selection approaches (fringes, BFS, DFS, MCTS),
            \item Embedding approach (Pre-trained + fine tuned LLM, GNN, Tree LSTM, DAG LSTM, Autoencoder)
            \item tactic selection (Decoder LLM, LSTM, MLP)
            \item tasks/ training approach ((end to end RL, supervised learning (premise selection, tactic prediction)))
        \end{itemize}
        \item We present a unified framework for experimentation in automating ITP. Includes
        \begin{itemize}
            \item Modular separation of data, experiments, models and environments.
            \item Framework uses wandb and pytorch lightning.
            Allows for easy reproducibility and general logging, checkpointing and data loading capability.
            \item HOL4 Reinforcement Learning environment, TacticZero algorithm (original, and extended for End-to-End embeddings, no current code available)
            \item HOL4 Premise selection task (novel)
            \item HOLStep Premise Selection task
            \item HOList Environment (Deepmath-light, easy to run, language agnostic)
            \item MIZAR 40 Premise Selection Task (no previous implementation)
        \end{itemize}
        \item Using this, we present a comparison of embedding architectures across these and show..
        \item modular and extensible central benchmark and experimentation platform.
    \end{itemize}

    \subsection{Extension to large scale verification projects}\label{subsec:extension-to-large-scale-verification-projects}
    Most work only on formalising pure maths, remains to be seen how it does on a larger project.


    \section{Progress and Timeline}\label{sec:progress-and-timeline}
    A summary of the current progress is given below:

    \begin{itemize}
        \item Reimplemented TacticZero algorithm.
        \item Generation of HOL4 premise selection pretraining dataset
        \item Implemented Graph Neural Network (GNN), Transformer, Structure Aware Attention (SAT) and novel Relation Attention embedding algorithms
        \item Tested performance of GNN and Transformer end to end embedding architectures on HOL4 TacticZero task.
        Demonstrated significant improvement (50\%+)
        \item Experimentation framework implemented which includes:
        \begin{itemize}
            \item Modular separation of data, experiments, models and environments.
            \item Framework uses wandb and pytorch lightning.
            Allows for easy reproducibility and general logging, checkpointing and data loading capability.
            \item HOL4 Reinforcement Learning environment, TacticZero algorithm (original, and extended for End-to-End embeddings)
            \item HOL4 Premise selection task
            \item HOLStep Premise Selection task
            \item HOList Environment
            \item MIZAR 40 Premise Selection Task
        \end{itemize}
        \item Refactored HOList benchmark to be easier to run, with PyTorch integration, and not need Google Dependencies.
    \end{itemize}

    \printbibliography

\end{document}